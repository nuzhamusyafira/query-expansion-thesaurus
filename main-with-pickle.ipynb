{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import string\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap dari digilib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paper_ori.pkl', 'rb') as f:\n",
    "    paper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_paper.pkl', 'rb') as f:\n",
    "    processed_paper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('thesaurus.pkl', 'rb') as f:\n",
    "    thesaurus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x for x in thesaurus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_query = 'pengolahan dokumen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olah', 'dokumen']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "query = init_query\n",
    "query = query.lower()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "query = query.translate(remove_punctuation_map)\n",
    "query = stopword.remove(query)\n",
    "query = query.split()\n",
    "query = [stemmer.stem(x) for x in query]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_result = []\n",
    "x = [' '.join(query)]\n",
    "paper_tfidf = vectorizer.fit_transform(x + processed_paper)\n",
    "q = paper_tfidf[0]\n",
    "result = cosine_similarity(paper_tfidf, q)\n",
    "idx = np.argsort(-result,axis=0).flatten()    \n",
    "final = [[num, y[0], x] for num, y in enumerate(result) if y[0] > 0.0]\n",
    "max_result += final\n",
    "max_result = sorted(max_result, key=lambda x: x[1], reverse=True)\n",
    "set_result = set()\n",
    "new_result = []\n",
    "for item in max_result:\n",
    "    if item[0] not in set_result:\n",
    "        set_result.add(item[0])\n",
    "        new_result.append(item)\n",
    "    else:\n",
    "        pass\n",
    "len(new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 14\n",
      "QUERY ['olah dokumen']\n",
      "RANCANG BANGUN SISTEM PELACAKAN DOKUMEN MEMANFAATKAN FUSE DAN SAMBA FILE-SYSTEM STUDI KASUS PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n",
      "Result 15\n",
      "QUERY ['olah dokumen']\n",
      "DESIGN OF DOCUMENT TRACKING SYSTEM UTILIZING FUSE AND SAMBA FILE-SYSTEM CASE STUDY IN PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n",
      "Result 32\n",
      "QUERY ['olah dokumen']\n",
      "RANCANG BANGUN SISTEM PELACAKAN DOKUMEN MEMANFAATKAN FUSE DAN SAMBA FILE-SYSTEM STUDI KASUS PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n",
      "Result 33\n",
      "QUERY ['olah dokumen']\n",
      "DESIGN OF DOCUMENT TRACKING SYSTEM UTILIZING FUSE AND SAMBA FILE-SYSTEM CASE STUDY IN PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in new_result[1:5]: \n",
    "    print('Result', x[0]) \n",
    "    print('QUERY', x[2]) \n",
    "    print(paper[x[0]-1][1]) \n",
    "    print(paper[x[0]-1][2][:200] + '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_res = [x[0]-1 for x in new_result[1:]]\n",
    "file = []\n",
    "for i,x in enumerate(paper):\n",
    "    if i in idx_res:\n",
    "        file.append([x[1],x[2],'ok_ok'])\n",
    "    else:\n",
    "        file.append([x[1],x[2],''])\n",
    "df = pd.DataFrame(file)\n",
    "df.to_excel('hasil/hasil ori ' +init_query+ '.xlsx', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olah', 'dokumen']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "query = init_query\n",
    "query = query.lower()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "query = query.translate(remove_punctuation_map)\n",
    "query = stopword.remove(query)\n",
    "query = query.split()\n",
    "query = [stemmer.stem(x) for x in query]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['olah dokumen'] ['olah akta'] ['olah arsip'] ['olah inskripsi'] ['olah kopi'] ['olah manuskrip'] ['olah tinggal'] ['olah piagam'] ['olah sahifah'] ['olah salin'] ['olah sertifikat'] ['olah surat'] ['olah tembus'] ['olah tindas'] ['akal dokumen'] ['akal akta'] ['akal arsip'] ['akal inskripsi'] ['akal kopi'] ['akal manuskrip'] ['akal tinggal'] ['akal piagam'] ['akal sahifah'] ['akal salin'] ['akal sertifikat'] ['akal surat'] ['akal tembus'] ['akal tindas'] ['canda dokumen'] ['canda akta'] ['canda arsip'] ['canda inskripsi'] ['canda kopi'] ['canda manuskrip'] ['canda tinggal'] ['canda piagam'] ['canda sahifah'] ['canda salin'] ['canda sertifikat'] ['canda surat'] ['canda tembus'] ['canda tindas'] ['cara dokumen'] ['cara akta'] ['cara arsip'] ['cara inskripsi'] ['cara kopi'] ['cara manuskrip'] ['cara tinggal'] ['cara piagam'] ['cara sahifah'] ['cara salin'] ['cara sertifikat'] ['cara surat'] ['cara tembus'] ['cara tindas'] ['daya dokumen'] ['daya akta'] ['daya arsip'] ['daya inskripsi'] ['daya kopi'] ['daya manuskrip'] ['daya tinggal'] ['daya piagam'] ['daya sahifah'] ['daya salin'] ['daya sertifikat'] ['daya surat'] ['daya tembus'] ['daya tindas'] ['gaya dokumen'] ['gaya akta'] ['gaya arsip'] ['gaya inskripsi'] ['gaya kopi'] ['gaya manuskrip'] ['gaya tinggal'] ['gaya piagam'] ['gaya sahifah'] ['gaya salin'] ['gaya sertifikat'] ['gaya surat'] ['gaya tembus'] ['gaya tindas'] ['laku dokumen'] ['laku akta'] ['laku arsip'] ['laku inskripsi'] ['laku kopi'] ['laku manuskrip'] ['laku tinggal'] ['laku piagam'] ['laku sahifah'] ['laku salin'] ['laku sertifikat'] ['laku surat'] ['laku tembus'] ['laku tindas'] ['lagu dokumen'] ['lagu akta'] ['lagu arsip'] ['lagu inskripsi'] ['lagu kopi'] ['lagu manuskrip'] ['lagu tinggal'] ['lagu piagam'] ['lagu sahifah'] ['lagu salin'] ['lagu sertifikat'] ['lagu surat'] ['lagu tembus'] ['lagu tindas'] ['laku dokumen'] ['laku akta'] ['laku arsip'] ['laku inskripsi'] ['laku kopi'] ['laku manuskrip'] ['laku tinggal'] ['laku piagam'] ['laku sahifah'] ['laku salin'] ['laku sertifikat'] ['laku surat'] ['laku tembus'] ['laku tindas'] ['muslihat dokumen'] ['muslihat akta'] ['muslihat arsip'] ['muslihat inskripsi'] ['muslihat kopi'] ['muslihat manuskrip'] ['muslihat tinggal'] ['muslihat piagam'] ['muslihat sahifah'] ['muslihat salin'] ['muslihat sertifikat'] ['muslihat surat'] ['muslihat tembus'] ['muslihat tindas'] ['perangai dokumen'] ['perangai akta'] ['perangai arsip'] ['perangai inskripsi'] ['perangai kopi'] ['perangai manuskrip'] ['perangai tinggal'] ['perangai piagam'] ['perangai sahifah'] ['perangai salin'] ['perangai sertifikat'] ['perangai surat'] ['perangai tembus'] ['perangai tindas'] ['ragam dokumen'] ['ragam akta'] ['ragam arsip'] ['ragam inskripsi'] ['ragam kopi'] ['ragam manuskrip'] ['ragam tinggal'] ['ragam piagam'] ['ragam sahifah'] ['ragam salin'] ['ragam sertifikat'] ['ragam surat'] ['ragam tembus'] ['ragam tindas'] ['tingkah laku dokumen'] ['tingkah laku akta'] ['tingkah laku arsip'] ['tingkah laku inskripsi'] ['tingkah laku kopi'] ['tingkah laku manuskrip'] ['tingkah laku tinggal'] ['tingkah laku piagam'] ['tingkah laku sahifah'] ['tingkah laku salin'] ['tingkah laku sertifikat'] ['tingkah laku surat'] ['tingkah laku tembus'] ['tingkah laku tindas'] ['ulah dokumen'] ['ulah akta'] ['ulah arsip'] ['ulah inskripsi'] ['ulah kopi'] ['ulah manuskrip'] ['ulah tinggal'] ['ulah piagam'] ['ulah sahifah'] ['ulah salin'] ['ulah sertifikat'] ['ulah surat'] ['ulah tembus'] ['ulah tindas'] "
     ]
    }
   ],
   "source": [
    "product_query = []\n",
    "list_synonym = []\n",
    "for x in query:\n",
    "    if x in words:\n",
    "        list_synonym.append(thesaurus[x])\n",
    "    else:\n",
    "        name = x\n",
    "        data = { \"q\": name }\n",
    "        encoded_data = urllib.parse.urlencode(data).encode(\"utf-8\")\n",
    "        content = urllib.request.urlopen(\"http://www.sinonimkata.com/search.php\", encoded_data)\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        try:\n",
    "            synonym = soup.find('td', attrs={'width': '90%'}).find_all('a')\n",
    "            synonym = [x.getText() for x in synonym]\n",
    "            thesaurus[x] = [x] + synonym\n",
    "            list_synonym.append(thesaurus[x])\n",
    "        except:\n",
    "            list_synonym.append([x])\n",
    "qs = []\n",
    "for x in itertools.product(*list_synonym):\n",
    "    x = [stemmer.stem(y) for y in x]\n",
    "    qs.append([' '.join(x)])\n",
    "for x in qs:\n",
    "    print(x, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_result = []\n",
    "for x in qs:\n",
    "    paper_tfidf = vectorizer.fit_transform(x + processed_paper)\n",
    "    q = paper_tfidf[0]\n",
    "    result = cosine_similarity(paper_tfidf, q)\n",
    "    idx = np.argsort(-result,axis=0).flatten()    \n",
    "    final = [[num, y[0], x] for num, y in enumerate(result) if y[0] > 0.0]\n",
    "    max_result += final\n",
    "max_result = sorted(max_result, key=lambda x: x[1], reverse=True)\n",
    "set_result = set()\n",
    "new_result = []\n",
    "for item in max_result:\n",
    "    if item[0] not in set_result:\n",
    "        set_result.add(item[0])\n",
    "        new_result.append(item)\n",
    "    else:\n",
    "        pass\n",
    "len(new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 15\n",
      "QUERY ['laku dokumen']\n",
      "RANCANG BANGUN SISTEM PELACAKAN DOKUMEN MEMANFAATKAN FUSE DAN SAMBA FILE-SYSTEM STUDI KASUS PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n",
      "Result 16\n",
      "QUERY ['laku dokumen']\n",
      "DESIGN OF DOCUMENT TRACKING SYSTEM UTILIZING FUSE AND SAMBA FILE-SYSTEM CASE STUDY IN PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n",
      "Result 33\n",
      "QUERY ['laku dokumen']\n",
      "RANCANG BANGUN SISTEM PELACAKAN DOKUMEN MEMANFAATKAN FUSE DAN SAMBA FILE-SYSTEM STUDI KASUS PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n",
      "Result 34\n",
      "QUERY ['laku dokumen']\n",
      "DESIGN OF DOCUMENT TRACKING SYSTEM UTILIZING FUSE AND SAMBA FILE-SYSTEM CASE STUDY IN PT. ANEKA TUNA INDONESIA\n",
      "Saat ini dokumen digital merupakan aspek penting dalam\n",
      "dunia bisnis. Dokumen digital memuat data-data penting\n",
      "perusahaan baik berupa data produksi sampai data keuangan.\n",
      "Dokumen digital dalam perusa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in new_result[1:5]: \n",
    "    print('Result', x[0]+1) \n",
    "    print('QUERY', x[2]) \n",
    "    print(paper[x[0]-1][1]) \n",
    "    print(paper[x[0]-1][2][:200] + '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_res = [x[0]-1 for x in new_result[1:]]\n",
    "file = []\n",
    "for i,x in enumerate(paper):\n",
    "    if i in idx_res:\n",
    "        file.append([x[1],x[2],'ok_ok'])\n",
    "    else:\n",
    "        file.append([x[1],x[2],''])\n",
    "df = pd.DataFrame(file)\n",
    "df.to_excel('hasil/hasil expansion ' +init_query+ '.xlsx', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
