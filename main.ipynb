{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import string\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap dari digilib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digilib_url = 'http://digilib.its.ac.id/publisher/51100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paper: 100%|████████████████████████████████| 20/20 [00:13<00:00,  1.45paper/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,docs_num,20):\n",
    "    page = urllib.request.urlopen(digilib_url)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    docs = soup.find_all('span', attrs={'class': 'style5'})\n",
    "    link = []\n",
    "    for x in docs:\n",
    "        try:\n",
    "            link.append(x.find('a').get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    for x in tqdm(link[:20], desc='paper', unit='paper'):\n",
    "        page = urllib.request.urlopen(x)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        try:\n",
    "            title = soup.find('h2', attrs={'class': 'isi'}).find('i').getText()\n",
    "            abstract = soup.find('span', attrs={'class': 'teks'}).find('p').getText()\n",
    "            paper.append([x, title, abstract])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in paper:\n",
    "    text = x[2]\n",
    "    text = text.lower()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    text = text.translate(remove_punctuation_map)\n",
    "    text = stopword.remove(text)\n",
    "    text = text.split()\n",
    "    text = [stemmer.stem(x) for x in text]\n",
    "    text = list(set(text))\n",
    "    words += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terkadang\n",
      "hyperellipsoid\n",
      "modul\n",
      "jejaring\n",
      "multi\n",
      "ke\n",
      "dariuji\n",
      "piksel\n",
      "itu\n",
      "berubahubah\n",
      "meister\n",
      "reliabilitas\n",
      "peers\n",
      "lowlevel\n",
      "mahalanobis\n",
      "erp\n",
      "belum\n",
      "masingmasing\n",
      "ekstraksi\n",
      "upa\n",
      "selfservice\n",
      "of\n",
      "thresholding\n",
      "hosting\n",
      "facebook\n",
      "fiturfitur\n",
      "sdk\n",
      "komputasi\n",
      "topsis\n",
      "skeletonisasi\n",
      "konektivitas\n",
      "cmeans\n",
      "algoritma\n",
      "its\n",
      "criteria\n",
      "tiga\n",
      "seringkali\n",
      "tiap\n",
      "9854\n",
      "turnbased\n",
      "25022\n",
      "sehingga\n",
      "kartu\n",
      "mcdm\n",
      "simkuri\n",
      "metered\n",
      "16203\n",
      "medis\n",
      "datadata\n",
      "sisip\n",
      "skalabilitas\n",
      "pengklasteran\n",
      "biru\n",
      "berbedabeda\n",
      "pooling\n",
      "kompetensi\n",
      "for\n",
      "pso\n",
      "mht\n",
      "platformasaservice\n",
      "smartphone\n",
      "inisial\n",
      "konstanta\n",
      "multiplatform\n",
      "mfcm\n",
      "fuzzyahp\n",
      "rpg\n",
      "beehivez\n",
      "orangorang\n",
      "tenantpenyewa\n",
      "troughput\n",
      "maupun\n",
      "subskenario\n",
      "paas\n",
      "bas\n",
      "teknologi\n",
      "menempatakan\n",
      "seharihari\n",
      "klaster\n",
      "telepon\n",
      "ratarata\n",
      "kurikulum\n",
      "jcrc\n",
      "146\n",
      "ini\n",
      "ujicoba\n",
      "missclassification\n",
      "multitenancy\n",
      "permasalahandengan\n",
      "vps\n",
      "games\n",
      "bluetooth\n",
      "kmeans\n",
      "isoiec\n"
     ]
    }
   ],
   "source": [
    "for x in words:\n",
    "    name = x\n",
    "    data = { \"q\": name }\n",
    "    encoded_data = urllib.parse.urlencode(data).encode(\"utf-8\")\n",
    "    content = urllib.request.urlopen(\"http://www.sinonimkata.com/search.php\", encoded_data)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    try:\n",
    "        synonym = soup.find('td', attrs={'width': '90%'}).find_all('a')\n",
    "        synonym = [x.getText() for x in synonym]\n",
    "        thesaurus[x] = synonym\n",
    "    except:\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
