{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import string\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate corpus for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap documents from digilib.its.ac.id\n",
    "\n",
    "docs_num = 400\n",
    "digilib_url = 'http://digilib.its.ac.id/publisher/51100/'\n",
    "paper = []\n",
    "for i in range(0,docs_num,20):\n",
    "    page = urllib.request.urlopen(digilib_url+str(i))\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    docs = soup.find_all('span', attrs={'class': 'style5'})\n",
    "    link = []\n",
    "    for x in docs:\n",
    "        try:\n",
    "            link.append(x.find('a').get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    for x in link[:20]:\n",
    "        clear_output(wait=True)\n",
    "        page = urllib.request.urlopen(x)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        try:\n",
    "            title = soup.find('h2', attrs={'class': 'isi'}).find('i').getText()\n",
    "            abstract = soup.find('span', attrs={'class': 'teks'}).find('p').getText()\n",
    "            paper.append([x, title, abstract])\n",
    "            display(x)\n",
    "        except:\n",
    "            pass\n",
    "print(\"Number of papers with abstract found: \" +str(len(paper))+ \" papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to 'corpus/paper.xlsx'\n",
    "\n",
    "print(\"Saving data to corpus/paper.xlsx..\")\n",
    "df = pd.DataFrame(paper)\n",
    "df.to_excel('corpus/paper.xlsx', header=False, index=False)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to 'pickle/paper.pkl'\n",
    "\n",
    "print(\"Saving data to pickle/paper.pkl..\")\n",
    "with open('pickle/paper.pkl', 'wb') as f:\n",
    "    pickle.dump(paper, f)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "words = []\n",
    "processed_paper = []\n",
    "for x in tqdm(paper, desc='paper', unit='paper'):\n",
    "    text = x[2]\n",
    "    text = text.lower()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    text = text.translate(remove_punctuation_map)\n",
    "    text = stopword.remove(text)\n",
    "    text = text.split()\n",
    "    text = [stemmer.stem(x) for x in text]\n",
    "    processed_paper.append(' '.join(text))\n",
    "    text = list(set(text))\n",
    "    words += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to 'corpus/processed_paper.xlsx'\n",
    "\n",
    "print(\"Saving data to corpus/processed_paper.xlsx..\")\n",
    "df = pd.DataFrame(processed_paper)\n",
    "df.to_excel('corpus/processed_paper.xlsx', header=False, index=False)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to 'pickle/processed_paper.pkl'\n",
    "\n",
    "print(\"Saving data to pickle/processed_paper.pkl..\")\n",
    "with open('pickle/processed_paper.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_paper, f)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save words to 'corpus/words.xlsx'\n",
    "\n",
    "print(\"Saving data to corpus/words.xlsx..\")\n",
    "df = pd.DataFrame(words)\n",
    "df.to_excel('corpus/words.xlsx', header=False, index=False)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save words to 'pickle/words.pkl'\n",
    "\n",
    "print(\"Saving data to pickle/words.pkl..\")\n",
    "with open('pickle/words.pkl', 'wb') as f:\n",
    "    pickle.dump(words, f)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap from sinonimkata.com\n",
    "\n",
    "thesaurus = {}\n",
    "words = list(set(words))\n",
    "for x in tqdm(words, desc='word', unit='word'):\n",
    "    name = x\n",
    "    data = { \"q\": name }\n",
    "    encoded_data = urllib.parse.urlencode(data).encode(\"utf-8\")\n",
    "    content = urllib.request.urlopen(\"http://www.sinonimkata.com/search.php\", encoded_data)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    try:\n",
    "        synonym = soup.find('td', attrs={'width': '90%'}).find_all('a')\n",
    "        synonym = [x.getText() for x in synonym]\n",
    "        thesaurus[x] = [x] + synonym\n",
    "    except:\n",
    "        thesaurus[x] = [name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to 'corpus/thesaurus.xlsx'\n",
    "\n",
    "print(\"Saving data to corpus/thesaurus.xlsx..\")\n",
    "df = pd.DataFrame(thesaurus)\n",
    "df.to_excel('corpus/thesaurus.xlsx', header=False, index=False)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to 'pickle/thesaurus.pkl'\n",
    "\n",
    "print(\"Saving data to pickle/thesaurus.pkl..\")\n",
    "with open('pickle/thesaurus.pkl', 'wb') as f:\n",
    "    pickle.dump(thesaurus, f)\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1. Query: 'pengembangan aplikasi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert query here\n",
    "\n",
    "init_query = 'pengembangan aplikasi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without query expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tf_idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "query = init_query\n",
    "query = query.lower()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "query = query.translate(remove_punctuation_map)\n",
    "query = stopword.remove(query)\n",
    "query = query.split()\n",
    "query = [stemmer.stem(x) for x in query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the query\n",
    "\n",
    "max_result = []\n",
    "x = [' '.join(query)]\n",
    "paper_tfidf = vectorizer.fit_transform(x + processed_paper)\n",
    "q = paper_tfidf[0]\n",
    "result = cosine_similarity(paper_tfidf, q)\n",
    "idx = np.argsort(-result,axis=0).flatten()    \n",
    "final = [[num, y[0], x] for num, y in enumerate(result) if y[0] > 0.0]\n",
    "max_result += final\n",
    "max_result = sorted(max_result, key=lambda x: x[1], reverse=True)\n",
    "set_result = set()\n",
    "new_result = []\n",
    "for item in max_result:\n",
    "    if item[0] not in set_result:\n",
    "        set_result.add(item[0])\n",
    "        new_result.append(item)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 5 results\n",
    "\n",
    "for x in new_result[1:6]: \n",
    "    print('Result', x[0]) \n",
    "    print('QUERY', x[2]) \n",
    "    print(paper[x[0]-1][1]) \n",
    "    print(paper[x[0]-1][2][:200] + '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_res = [x[0]-1 for x in new_result[1:]]\n",
    "file = []\n",
    "for i,x in enumerate(paper):\n",
    "    if i in idx_res:\n",
    "        file.append([x[1],x[2],'ok_ok'])\n",
    "    else:\n",
    "        file.append([x[1],x[2],''])\n",
    "df = pd.DataFrame(file)\n",
    "df.to_excel('hasil/hasil ori ' +init_query+ '.xlsx', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With query expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tf-idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "query = init_query\n",
    "query = query.lower()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "query = query.translate(remove_punctuation_map)\n",
    "query = stopword.remove(query)\n",
    "query = query.split()\n",
    "query = [stemmer.stem(x) for x in query]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_query = []\n",
    "list_synonym = []\n",
    "for x in query:\n",
    "    if x in words:\n",
    "        list_synonym.append(thesaurus[x])\n",
    "    else:\n",
    "        name = x\n",
    "        data = { \"q\": name }\n",
    "        encoded_data = urllib.parse.urlencode(data).encode(\"utf-8\")\n",
    "        content = urllib.request.urlopen(\"http://www.sinonimkata.com/search.php\", encoded_data)\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        try:\n",
    "            synonym = soup.find('td', attrs={'width': '90%'}).find_all('a')\n",
    "            synonym = [x.getText() for x in synonym]\n",
    "            thesaurus[x] = [x] + synonym\n",
    "            list_synonym.append(thesaurus[x])\n",
    "        except:\n",
    "            list_synonym.append([x])\n",
    "qs = []\n",
    "for x in itertools.product(*list_synonym):\n",
    "    x = [stemmer.stem(y) for y in x]\n",
    "    qs.append([' '.join(x)])\n",
    "for x in qs:\n",
    "    print(x, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result = []\n",
    "for x in qs:\n",
    "    paper_tfidf = vectorizer.fit_transform(x + processed_paper)\n",
    "    q = paper_tfidf[0]\n",
    "    result = cosine_similarity(paper_tfidf, q)\n",
    "    idx = np.argsort(-result,axis=0).flatten()    \n",
    "    final = [[num, y[0], x] for num, y in enumerate(result) if y[0] > 0.0]\n",
    "    max_result += final\n",
    "max_result = sorted(max_result, key=lambda x: x[1], reverse=True)\n",
    "set_result = set()\n",
    "new_result = []\n",
    "for item in max_result:\n",
    "    if item[0] not in set_result:\n",
    "        set_result.add(item[0])\n",
    "        new_result.append(item)\n",
    "    else:\n",
    "        pass\n",
    "len(new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in new_result[1:5]: \n",
    "    print('Result', x[0]+1) \n",
    "    print('QUERY', x[2]) \n",
    "    print(paper[x[0]-1][1]) \n",
    "    print(paper[x[0]-1][2][:200] + '...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_res = [x[0]-1 for x in new_result[1:]]\n",
    "file = []\n",
    "for i,x in enumerate(paper):\n",
    "    if i in idx_res:\n",
    "        file.append([x[1],x[2],'ok_ok'])\n",
    "    else:\n",
    "        file.append([x[1],x[2],''])\n",
    "df = pd.DataFrame(file)\n",
    "df.to_excel('hasil/hasil expansion ' +init_query+ '.xlsx', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
